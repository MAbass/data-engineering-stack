{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75f1481",
   "metadata": {},
   "source": [
    "# 4. Structured Streaming with the Retail Data-Set #\n",
    "Use the by-day retail data-set to simulate a daily feed of data to be read by a Spark Structured Streaming job.   \n",
    "\n",
    "Each file that is read in by the `spark.ReadStream` method uses the option *maxFilesPerTrigger=1* to trigger an update to the streaming dataframe contents.  \n",
    "  \n",
    "A transformation called `purchaseByCustomerPerHour` is mapped to the streaming dataframe with a query transformation to show the top 5 customers and their spend so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a73600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/08 17:12:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-nb-4-streaming\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a237be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Streaming functions for creating window and defining a column (col) to calculate the window over\n",
    "#from pyspark.sql.functions import window, column, desc, col\n",
    "from pyspark.sql.functions import window,  col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f9822",
   "metadata": {},
   "source": [
    "First, get a sample of the data to be processed in a continuous stream.  \n",
    "+ We have to assume that future data will have the same schema.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfb3ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_sample = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/opt/workspace/datain/retail-data/by-day/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2885257d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(InvoiceNo='536365', StockCode='85123A', Description='WHITE HANGING HEART T-LIGHT HOLDER', Quantity=6, InvoiceDate=datetime.datetime(2010, 12, 1, 8, 26), UnitPrice=2.55, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='71053', Description='WHITE METAL LANTERN', Quantity=6, InvoiceDate=datetime.datetime(2010, 12, 1, 8, 26), UnitPrice=3.39, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='84406B', Description='CREAM CUPID HEARTS COAT HANGER', Quantity=8, InvoiceDate=datetime.datetime(2010, 12, 1, 8, 26), UnitPrice=2.75, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='84029G', Description='KNITTED UNION FLAG HOT WATER BOTTLE', Quantity=6, InvoiceDate=datetime.datetime(2010, 12, 1, 8, 26), UnitPrice=3.39, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='84029E', Description='RED WOOLLY HOTTIE WHITE HEART.', Quantity=6, InvoiceDate=datetime.datetime(2010, 12, 1, 8, 26), UnitPrice=3.39, CustomerID=17850.0, Country='United Kingdom')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4d561",
   "metadata": {},
   "source": [
    "#### Create a Streaming Dataframe ####\n",
    "The Streaming Dataframe is created with a Schema based on the data sample we took earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08448f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Schema from the data_sample\n",
    "staticSchema = data_sample.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37cdca9",
   "metadata": {},
   "source": [
    "Create a streaming dataframe using `maxFilesPerTrigger` to trigger an update to the dataframe on file-by-file basis.  The `header` option caters for (assumes) headers on each file that is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e43e3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create a Streaming DataFrame based on a Static Schema from the data_sample\n",
    "streamingDataFrame = spark.readStream.schema(staticSchema).option(\"maxFilesPerTrigger\",1).format(\"csv\").option(\"header\", \"true\")\\\n",
    "       .load(\"/opt/workspace/datain/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf40ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the stream status\n",
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e1415",
   "metadata": {},
   "source": [
    "A streaming transformation `purchaseByCustomerPerHour` is created below; This applies a dataframe transformation which groups the data by *CustomerID* and *InvoiceDate* and sums the \"total_cost\" of *UnitPrice* x *Quantity*.  \n",
    "\n",
    "The `window` function is used to define the size of window over which this summary is provided - in this case a 1-day window of time.  This means we get to see which customers have bought the most *in a single day*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "804e1133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformation that sum's customer purchase per hour\n",
    "purchaseByCustomerPerHour = streamingDataFrame.selectExpr(\"CustomerID\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\") \\\n",
    "                                              .groupBy(col(\"CustomerID\"), window(col(\"InvoiceDate\"), \"1 day\")) \\\n",
    "                                              .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944c646",
   "metadata": {},
   "source": [
    "Create an in-memory structure called `customer_purchases` that  is written to with the output from the streaming transformation. This can be queried using Spark SQL to see the latest result-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "305ea437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fe275ff1e10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==================================>                   (129 + 2) / 200]"
     ]
    }
   ],
   "source": [
    "# Generate updates to an in-memory table after each trigger\n",
    "purchaseByCustomerPerHour.writeStream.format(\"memory\").queryName(\"customer_purchases\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecdefb1",
   "metadata": {},
   "source": [
    "Query `customer_purchases` to see the latest view of which customers have spent the most in a single day.  The results change as new retail data day-files are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c635946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==========================================>           (158 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerID|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|      null|[2010-12-01 00:00...|12584.299999999988|\n",
      "|   15061.0|[2010-12-02 00:00...| 9407.339999999998|\n",
      "|   13777.0|[2010-12-01 00:00...|           6585.16|\n",
      "|   17850.0|[2010-12-02 00:00...|3891.8699999999985|\n",
      "|   16029.0|[2010-12-01 00:00...|           3702.12|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d06ec0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerID|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|      null|[2010-12-03 00:00...| 23021.99999999999|\n",
      "|      null|[2010-12-01 00:00...|12584.299999999988|\n",
      "|   15061.0|[2010-12-02 00:00...| 9407.339999999998|\n",
      "|   13777.0|[2010-12-01 00:00...|           6585.16|\n",
      "|   17850.0|[2010-12-02 00:00...|3891.8699999999985|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:=========================>                             (92 + 2) / 200]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ddf7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468599de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
