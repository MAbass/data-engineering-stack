{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac3229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, regexp_extract, reverse, regexp_replace\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "879db3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_url = \"mongodb://192.168.1.7:27018/streaming.calls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6081dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install elasticsearch-hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6fe655",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\n",
    "        'PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,org.elasticsearch:elasticsearch-hadoop:7.11.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546d5f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "org.elasticsearch#elasticsearch-hadoop added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b0d13bb6-fdf9-4785-8d31-e3c91f98e34c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "\tfound org.elasticsearch#elasticsearch-hadoop;7.11.0 in central\n",
      "\tfound commons-logging#commons-logging;1.1.1 in central\n",
      "\tfound commons-httpclient#commons-httpclient;3.0.1 in central\n",
      "\tfound commons-codec#commons-codec;1.4 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound org.apache.hive#hive-service;1.2.1 in central\n",
      "\tfound org.apache.hive#hive-exec;1.2.1 in central\n",
      "\tfound org.apache.hive#hive-metastore;1.2.1 in central\n",
      "\tfound org.apache.pig#pig;0.15.0 in central\n",
      "\tfound org.apache.spark#spark-yarn_2.11;2.3.0 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.12 in central\n",
      "\tfound org.apache.storm#storm-core;1.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client;2.7.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;2.7.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-core;2.7.6 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.8.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.8.8 in central\n",
      "\tfound joda-time#joda-time;1.6 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 525ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.4 from central in [default]\n",
      "\tcommons-httpclient#commons-httpclient;3.0.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\tjoda-time#joda-time;1.6 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client;2.7.6 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;2.7.6 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-core;2.7.6 from central in [default]\n",
      "\torg.apache.hive#hive-exec;1.2.1 from central in [default]\n",
      "\torg.apache.hive#hive-metastore;1.2.1 from central in [default]\n",
      "\torg.apache.hive#hive-service;1.2.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.pig#pig;0.15.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-yarn_2.11;2.3.0 from central in [default]\n",
      "\torg.apache.storm#storm-core;1.0.6 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.8.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.8.8 from central in [default]\n",
      "\torg.elasticsearch#elasticsearch-hadoop;7.11.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.12 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   31  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b0d13bb6-fdf9-4785-8d31-e3c91f98e34c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/8ms)\n",
      "23/08/09 21:43:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-streaming\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.mongodb.output.uri\", mongo_url).\\\n",
    "        config(\"spark.mongodb.input.uri\", mongo_url).\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ddcd5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_row_in_mongo(batch_df, batch_id):\n",
    "    batch_df.write.format(\"mongo\").mode(\"append\").save()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc85daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "        .option(\"subscribe\", \"CDR_APPEL\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load() \\\n",
    "        .selectExpr(\"cast(key as string) key\", \"cast(value as string) value\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00124365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "865ae878",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stream = kafka_df.select(\n",
    "    split(kafka_df.value, \"\\\\|\").alias(\"data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b795a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66637894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la structure de données\n",
    "schema = \"Anumber STRING, Bnumber STRING, CallDate STRING, SiteName STRING, Coordinate STRING\"\n",
    "data_df = data_stream.selectExpr(\"data[0] AS Anumber\", \"data[1] AS Bnumber\", \"data[2] AS CallDate\", \"data[3] AS SiteName\", \"data[4] AS Coordinate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6bb3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "447eba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les données pour ignorer la ligne avec certaine Anumber\n",
    "filtered_stream = data_df.filter(~col(\"Anumber\").contains(\"Anumber\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81a97452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f122815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans cet exemple, nous remplaçons la virgule par un espace\n",
    "replace_stream = filtered_stream.withColumn(\"Coordinate\", regexp_replace(col(\"Coordinate\"), \"\\\\{\", \"\"))\n",
    "replace_stream = replace_stream.withColumn(\"Coordinate\", regexp_replace(col(\"Coordinate\"), \"\\\\}\", \"\"))\n",
    "# replace_stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "054e0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_stream = replace_stream.withColumn(\"Coordinate\", reverse(col(\"Coordinate\")))\n",
    "# filtered_stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e327c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the \"name\" column into first_name and last_name columns\n",
    "split_df = filtered_stream.withColumn(\"split_values\", split(col(\"Coordinate\"), \",\")) \\\n",
    "             .withColumn(\"Latitude\", col(\"split_values\")[0]) \\\n",
    "             .withColumn(\"Longitude\", col(\"split_values\")[1]) \\\n",
    "             .drop(\"split_values\")\\\n",
    "             .drop(\"Coordinate\")\n",
    "\n",
    "# split_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e69ce95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted_df = split_df.withColumn(\"Latitude\", col(\"Latitude\").cast(FloatType()))\n",
    "# converted_df = converted_df.withColumn(\"Longitude\", col(\"Longitude\").cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e841abb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/09 21:44:10 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4ea7c2a8-0c04-4984-a798-069d084ac07b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/08/09 23:19:39 ERROR TaskSchedulerImpl: Lost executor 1 on 172.24.0.10: worker lost\n",
      "23/08/09 23:19:39 ERROR TaskSchedulerImpl: Lost executor 0 on 172.24.0.11: worker lost\n",
      "23/08/09 23:53:05 WARN KafkaOffsetReader: Error in attempt 1 getting Kafka offsets: \n",
      "org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition CDR_APPEL-0 could be determined\n",
      "23/08/10 00:47:49 ERROR TaskSchedulerImpl: Lost executor 2 on 172.24.0.10: worker lost\n",
      "23/08/10 00:47:49 ERROR TaskSchedulerImpl: Lost executor 3 on 172.24.0.11: worker lost\n",
      "23/08/10 01:09:41 ERROR TaskSchedulerImpl: Lost executor 4 on 172.24.0.11: worker lost\n",
      "23/08/10 01:09:41 ERROR TaskSchedulerImpl: Lost executor 5 on 172.24.0.10: worker lost\n",
      "23/08/10 03:21:29 ERROR TaskSchedulerImpl: Lost executor 6 on 172.24.0.11: worker lost\n",
      "23/08/10 03:21:29 ERROR TaskSchedulerImpl: Lost executor 7 on 172.24.0.10: worker lost\n",
      "23/08/10 06:08:07 ERROR TaskSchedulerImpl: Lost executor 9 on 172.24.0.10: worker lost\n",
      "23/08/10 06:08:07 ERROR TaskSchedulerImpl: Lost executor 8 on 172.24.0.11: worker lost\n",
      "23/08/10 07:58:22 ERROR TaskSchedulerImpl: Lost executor 10 on 172.24.0.11: worker lost\n",
      "23/08/10 07:58:22 ERROR TaskSchedulerImpl: Lost executor 11 on 172.24.0.10: worker lost\n",
      "23/08/10 08:45:26 ERROR TaskSchedulerImpl: Lost executor 13 on 172.24.0.10: worker lost\n",
      "23/08/10 08:45:26 ERROR TaskSchedulerImpl: Lost executor 12 on 172.24.0.11: worker lost\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (\u001b[43msplit_df\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstreaming\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_row_in_mongo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/streaming.py:103\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m/usr/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(split_df\n",
    "     .writeStream\n",
    "     .queryName(\"streaming\")\n",
    "     .foreachBatch(write_row_in_mongo)\n",
    "     .start()\n",
    "     .awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d4d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fec58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824b4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
